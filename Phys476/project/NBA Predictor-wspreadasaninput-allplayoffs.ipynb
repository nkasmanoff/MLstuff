{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have at my disposal the box score of every nba game since 2003. Planning to predict the winner, spread, and over under .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import dependences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense\n",
    "from pandas import get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_network(FILENAME,sklearn=False,keras=False,normalize=True,overunder=False,spread=False,moneyline=False):\n",
    "    from pandas import read_csv,get_dummies\n",
    "    import numpy as np\n",
    "    from sklearn import cross_validation\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the csv input of all the box scores, arrange it such that the home and away teams are lined up, \n",
    "    unnecessary columns removed, and hot encoding is done. Other stuff too probably. Such as normalization, but I \n",
    "    didn't do that!\n",
    "    \n",
    "    Note that this data has already been doctored from its original form, taking out most unnecessary columns but\n",
    "    those could be useful later on.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FILENAME : file\n",
    "        The csv of the data.\n",
    "        \n",
    "    Along with all the options as bools (not properly documented yet sorry)\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    model : object\n",
    "        MLP which can predict the outcome of NBA games\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Read in file, remove attempted and # and only account for % since that's more predictive in nature. \n",
    "    #*retrospectively that doesn't make sense, could be worth changing!\n",
    "    data = read_csv(FILENAME) \n",
    "    data['3P%'] = np.divide(data['3P'].values,data['3PA'].values) \n",
    "    del data['3P'],data['3PA']\n",
    "    data['FG%'] = np.divide(data['FG'].values,data['FGA'].values)\n",
    "    del data['FG'],data['FGA']\n",
    "    data['FT%'] = np.divide(data['FT'].values,data['FTA'].values)\n",
    "    del data['Unnamed: 0'],data['GAME_ID'],data['Date'],data['Home'],data['Away'],data['PLUS_MINUS'],data['TOTAL']\n",
    "    del data['FT'],data['FTA']\n",
    "    data = get_dummies(data)\n",
    "\n",
    "    #print(data)\n",
    "    \n",
    "\n",
    "    dat = []\n",
    "    \n",
    "    #reshape the dataset so now each colummn has roadstats and homestats concatenated into the same row, used for NN \n",
    "    \n",
    "    for i in range(len(data.values)):\n",
    "        data.values[i] = np.reshape(data.values[i],newshape=[1,len(data.values[i])])\n",
    "    for p in range(int(len(data.values)/2)):\n",
    "        fullboxgame = np.concatenate((data.values[2*p],data.values[(2*p)+1]))\n",
    "        dat.append(fullboxgame)\n",
    "    \n",
    "    #convert list to array, now possible to array operations previously not possible\n",
    "    dat = np.array(dat)   \n",
    "    \n",
    "    openingspreadS = dat[:,8] #what the predicted spread of ther game was. \n",
    "    roadpts = dat[:,7]       #column of all the points scored by road team \n",
    "    homepts = dat[:,52]\n",
    "    endspreadS = roadpts-homepts  #all the final spreads of the game\n",
    "            #concatenating all the arrays, looks messy but explanation doen in another nb. \n",
    "    x1 = dat[:,0:7] #road offensive rebounds to blocks\n",
    "    x2 = dat[:,9:42] # road 3p% to team name (hot encoded)\n",
    "\n",
    "    x3 = dat[:,45:52] #home offensive rebounds to blocks\n",
    "    x4  =  dat[:,54:87] #home 3p% to hot encoded team name   \n",
    "                      \n",
    "    x5 = dat[:,8]              \n",
    "    X1 = np.concatenate((x1,x2),axis=1)\n",
    "    X2 = np.concatenate((x3,x4),axis=1)\n",
    "    X3 = np.concatenate((X1,X2),axis=1)\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    if overunder:\n",
    "        X = X3\n",
    "        #[OVER,PUSH,UNDER]\n",
    "        y = dat[:,42:45]\n",
    "        #save outcomes for all, easy to show over under. \n",
    "\n",
    "    \n",
    "    if spread:\n",
    "        #include initial spread of the game. \n",
    "        X = np.column_stack((X3,x5))\n",
    "\n",
    "        for j in range(len(endspreadS)):  \n",
    "            openspread = openingspreadS[j]\n",
    "       # print(\"this is the spread of the road team \" + str(openspread))\n",
    "            endspread = endspreadS[j]\n",
    "       # print(\"the road team won by  .. \" + str(endspread))\n",
    "       # if endspread>openspread:\n",
    "        #    y.append(np.array([0,1,0]))  #OK, now make sure this is formateed properly!\n",
    "            if openspread + endspread <0:\n",
    "                y.append(np.array([0,1,0]))  #home team covered\n",
    "            elif openspread + endspread >0:\n",
    "                y.append(np.array([1,0,0]))  #road covered\n",
    "            else: \n",
    "                y.append(np.array([0,0,1]))  #push!\n",
    "\n",
    "    \n",
    "    if moneyline:\n",
    "        X = X3\n",
    "        for j in range(len(endspreadS)):  \n",
    "            if endspreadS[j]<0:\n",
    "                #means the home team had more points\n",
    "                y.append(np.array([0,1]))\n",
    "            else:\n",
    "                y.append(np.array([1,0])) #alternatively, a road team victory. \n",
    "          \n",
    "\n",
    "    #Now I iterated over all these, and hot encoded the labels of each to see whether or not the spread was covered\n",
    "    #and by what team. \n",
    "\n",
    "\n",
    "        \n",
    "    y = np.array(y)  #same explanation as above\n",
    "                         \n",
    "    #since everything got out of order I have to mash it together myself. \n",
    "    if normalize:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "    X_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.27)\n",
    "    #print((X[0]))\n",
    "    #print(np.shape(X[0]))\n",
    "\n",
    "    #now to construct a model \n",
    "    if sklearn: \n",
    "        model = MLPClassifier()\n",
    "        model.shuffle = True\n",
    "        model.batch_size = 25\n",
    "    #model.n_layers_ = 1000000\n",
    "    #model.n_outputs_= 1000000\n",
    "    #These don't do anything, have to adjust the layers in some different way! Keras is useful for this.\n",
    "        model.fit(X_train,y_train)\n",
    "        print(model.score(X_test,y_test))\n",
    "    if keras:\n",
    "        print(\"keras NN goes here\")\n",
    "        model=Sequential()\n",
    "        model.add(Dense(22,input_dim=np.shape(X)[1],activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(50,activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(22,activation='relu'))\n",
    "\n",
    "        model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.fit(X_train,y_train,batch_size=40,epochs=20,validation_split=.2)\n",
    "        scores = model.evaluate(X_test,y_test)\n",
    "        print(scores[1])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model,scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbapredictorOU,scaler = make_network('NBADATA.csv',sklearn=True,keras=False,normalize=True,overunder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.831918505942275\n"
     ]
    }
   ],
   "source": [
    "nbapredictorSPREAD,scaler = make_network('NBADATA.csv',sklearn=True,keras=False,normalize=True,spread=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbapredictorML,scaler = make_network('NBADATA.csv',sklearn=True,keras=False,normalize=True,moneyline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function I have is for turning the current nba team statistics (either over the entire season or some stretch) into an array of the same shape and information as the one used for the box scores above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_splits(TEAMABR,ten=True,twenty=False,regseason=False):\n",
    "    \"\"\"returns the splits of a team over the past N days. Will consider changing this to a from - to thing for different dates. \n",
    "    \n",
    "    Designated splits are available, with more to come such as H2H against a certain team\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from nba_py import team\n",
    "    teams = team.TeamList()\n",
    "    teamids = teams.info()\n",
    "    teamids = teamids[:-15]\n",
    "    teamids = teamids[['TEAM_ID','ABBREVIATION']]\n",
    "\n",
    "    teamids = teamids.rename(index=str, columns={\"ABBREVIATION\": \"Team\"})\n",
    "    teamids = teamids.replace('BKN','BRK')\n",
    "    teamids = teamids.sort_values('Team')\n",
    "\n",
    "    TEAM_ID = teamids.loc[teamids['Team'] == TEAMABR].values[0,0]\n",
    "    teamids = pd.get_dummies(teamids)\n",
    "    teamarray = teamids.loc[teamids['TEAM_ID'] == TEAM_ID].values[0,1:]\n",
    "\n",
    "\n",
    "    TEAM = team.TeamLastNGamesSplits(team_id=TEAM_ID)\n",
    "    if ten:\n",
    "        df = TEAM.last10()\n",
    "    if twenty:\n",
    "        df = TEAM.last20()\n",
    "    if regseason:\n",
    "        TEAM = team.TeamGeneralSplits(team_id=TEAM_ID)\n",
    "        df = TEAM.overall()\n",
    "   # if five:\n",
    "        #df = TEAM\n",
    "    \n",
    "    df = df[['OREB','DREB','REB','PF','STL','TOV','BLK','FG3_PCT','FG_PCT','FT_PCT']].values\n",
    "    \n",
    "    teamsplits = np.concatenate((df[0],teamarray))\n",
    "    return teamsplits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bos = get_splits('BOS')\n",
    "cle = get_splits('CLE')\n",
    "hou = get_splits('HOU')\n",
    "gsw = get_splits('GSW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spread_game_maker(roadteam,hometeam,spread,scaler):\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"This includes the option to include the spread (OF THE ROAD TEAM), which isn't the case for the other ones. \n",
    "    \n",
    "        After creating a properly formated table, this concats the desired teams so they can be predicted. \n",
    "        Based on get team index # based on output of predictor, and make it the input for stats ie GSW are stats[0].\n",
    "        and so on!\n",
    "    \"\"\"\n",
    "    spread = np.array([spread])\n",
    "    game = np.concatenate((roadteam,hometeam))\n",
    "    game = np.concatenate((game,spread))\n",
    "    game = [game]\n",
    "    game = scaler.transform(game)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conference Finals\n",
    "\n",
    "clebos1 = spread_game_maker(cle,bos,-1,scaler)\n",
    "clebos2 = spread_game_maker(cle,bos,6,scaler)\n",
    "clebos3 = spread_game_maker(bos,cle,7,scaler)\n",
    "clebos4 = spread_game_maker(bos,cle,6.5,scaler)\n",
    "#clebos3\n",
    "\n",
    "gswhou1 = spread_game_maker(gsw,hou,1.5,scaler)\n",
    "gswhou2 = spread_game_maker(gsw,hou,2,scaler)\n",
    "gswhou3 = spread_game_maker(hou,gsw,7.5,scaler)\n",
    "gswhou4 = spread_game_maker(hou,gsw,8,scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbapredictorSPREAD.predict(clebos4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = get_splits('IND')\n",
    "cle = get_splits('CLE')\n",
    "was = get_splits('WAS')\n",
    "tor = get_splits('TOR')\n",
    "mia = get_splits('MIA')\n",
    "phi = get_splits('PHI')\n",
    "mil = get_splits('MIL')\n",
    "bos = get_splits('BOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minn = get_splits('MIN')\n",
    "hou = get_splits('HOU')\n",
    "uta = get_splits('UTA')\n",
    "okc = get_splits('OKC')\n",
    "nop = get_splits('NOP')\n",
    "por = get_splits('POR')\n",
    "sas = get_splits('SAS')\n",
    "gsw = get_splits('GSW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indcle1 = spread_game_maker(ind,cle,7.5,scaler)\n",
    "indcle2 = spread_game_maker(ind,cle,8,scaler)\n",
    "cleind3 = spread_game_maker(cle,ind,1.5,scaler)\n",
    "cleind4 = spread_game_maker(cle,ind,-1.5,scaler)\n",
    "indcle5 = spread_game_maker(ind,cle,6.5,scaler)\n",
    "cleind6 = spread_game_maker(cle,ind,-1.5,scaler)\n",
    "indcle7 = spread_game_maker(ind,cle,5,scaler)\n",
    "\n",
    "\n",
    "wastor1 = spread_game_maker(was,tor,7.5,scaler)\n",
    "wastor2 = spread_game_maker(was,tor,7,scaler)\n",
    "wastor3 = spread_game_maker(tor,was,1,scaler)\n",
    "wastor4 = spread_game_maker(tor,was,-1.5,scaler)\n",
    "wastor5 = spread_game_maker(was,tor,7,scaler)\n",
    "wastor6 = spread_game_maker(tor,was,-2,scaler)\n",
    "\n",
    "miaphi1 = spread_game_maker(mia,phi,6,scaler)\n",
    "miaphi2 = spread_game_maker(mia,phi,6.5,scaler)\n",
    "miaphi3 = spread_game_maker(phi,mia,-2.5,scaler)\n",
    "miaphi4 = spread_game_maker(phi,mia,-4,scaler)\n",
    "miaphi5 = spread_game_maker(mia,phi,10,scaler)\n",
    "\n",
    "milbos1 = spread_game_maker(mil,bos,4,scaler)\n",
    "milbos2 = spread_game_maker(mil,bos,13,scaler)\n",
    "milbos3 = spread_game_maker(bos,mil,4.5,scaler)\n",
    "milbos4 = spread_game_maker(bos,mil,6,scaler)\n",
    "milbos5 = spread_game_maker(mil,bos,4.5,scaler)\n",
    "milbos6 = spread_game_maker(bos,mil,4.5,scaler)\n",
    "milbos7 = spread_game_maker(mil,bos,5,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#western conference round 1 matchups\n",
    "\n",
    "minhou1 = spread_game_maker(minn,hou,11.5,scaler)\n",
    "minhou2 = spread_game_maker(minn,hou,10.5,scaler)\n",
    "minhou3 = spread_game_maker(hou,minn,-6,scaler)\n",
    "minhou4 = spread_game_maker(hou,minn,-6,scaler)\n",
    "minhou5 = spread_game_maker(minn,hou,12,scaler)\n",
    "\n",
    "utaokc1 = spread_game_maker(uta,okc,4,scaler)\n",
    "utaokc2 = spread_game_maker(uta,okc,3.5,scaler)\n",
    "utaokc3 = spread_game_maker(okc,uta,5,scaler)\n",
    "utaokc4 = spread_game_maker(okc,uta,4.5,scaler)\n",
    "utaokc5 = spread_game_maker(uta,okc,2.5,scaler)\n",
    "utaokc6 = spread_game_maker(okc,uta,7,scaler)\n",
    "\n",
    "noppor1 = spread_game_maker(nop,por,5,scaler)\n",
    "noppor2 = spread_game_maker(nop,por,6,scaler)\n",
    "noppor3 = spread_game_maker(por,nop,4,scaler)\n",
    "noppor4 = spread_game_maker(por,nop,7,scaler)\n",
    "\n",
    "\n",
    "sasgsw1 = spread_game_maker(sas,gsw,8,scaler)\n",
    "sasgsw2 = spread_game_maker(sas,gsw,9,scaler)\n",
    "sasgsw3 = spread_game_maker(gsw,sas,-3.5,scaler)\n",
    "sasgsw4 = spread_game_maker(gsw,sas,-5.5,scaler)\n",
    "sasgsw5 = spread_game_maker(sas,gsw,11,scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Round 2 Matchups\n",
    "\n",
    "cletor1 = spread_game_maker(cle,tor,7,scaler)\n",
    "cletor2 = spread_game_maker(cle,tor,6.5,scaler)\n",
    "cletor3 = spread_game_maker(tor,cle,4.5,scaler)\n",
    "cletor4 = spread_game_maker(tor,cle,5,scaler)\n",
    "\n",
    "phibos1 = spread_game_maker(phi,bos,-5,scaler)\n",
    "phibos2 = spread_game_maker(phi,bos,-3.5,scaler)\n",
    "phibos3 = spread_game_maker(bos,phi,1.5,scaler)\n",
    "phibos4 = spread_game_maker(bos,phi,7,scaler)\n",
    "phibos5 = spread_game_maker(phi,bos,1.5,scaler)\n",
    "\n",
    "\n",
    "utahou1 =  spread_game_maker(uta,hou,11.5,scaler)\n",
    "utahou2 =  spread_game_maker(uta,hou,11,scaler)\n",
    "utahou3 =  spread_game_maker(hou,uta,-4.5,scaler)\n",
    "utahou4 =  spread_game_maker(hou,uta,-5.5,scaler)\n",
    "utahou5 =  spread_game_maker(uta,hou,11.5,scaler)\n",
    "\n",
    "\n",
    "nopgsw1 = spread_game_maker(nop,gsw,7,scaler)\n",
    "nopgsw2 = spread_game_maker(nop,gsw,11,scaler)\n",
    "nopgsw3 = spread_game_maker(gsw,nop,-4.5,scaler)\n",
    "nopgsw4 = spread_game_maker(gsw,nop,-5.5,scaler)\n",
    "nopgsw5 = spread_game_maker(nop,gsw,12,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorSPREAD.predict(clebos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x x y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "23+12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "35-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorSPREAD.predict(clebos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OU_ML_game_maker(roadteam,hometeam,scaler):\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"\n",
    "        After creating a properly formated table, this concats the desired teams so they can be predicted. \n",
    "        Based on get team index # based on output of predictor, and make it the input for stats ie GSW are stats[0].\n",
    "        and so on!\n",
    "    \"\"\"\n",
    "    game = np.concatenate((roadteam,hometeam))\n",
    "    game.append(spread)\n",
    "    game = [game]\n",
    "    game = scaler.transform(game)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clebos = game_maker(cle,bos,scaler)\n",
    "gswhou = spread_game_maker(gsw,hou,2,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorOU.predict(gswhou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorML.predict(gswhou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorSPREAD.predict(gswhou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ This is differnet from the original prediction, which said it was going to be Golden State. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorOU.predict(clebos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictorML.predict(clebos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
