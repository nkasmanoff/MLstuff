{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my most up to date and as annotated as possible script. For the data set, have it in you local directory (It's uploaded to github too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import dependences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "np.random.seed(7) #why random?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(FILENAME,sklearn=False,keras=False):\n",
    "    from pandas import read_csv,get_dummies\n",
    "    import numpy as np\n",
    "    from sklearn import cross_validation\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the csv input of all the box scores, arrange it such that the home and away teams are lined up, \n",
    "    unnecessary columns removed, and hot encoding is done. Other stuff too probably. Such as normalization, but I \n",
    "    didn't do that!\n",
    "    \n",
    "    Note that this data has already been doctored from its original form, taking out most unnecessary columns but\n",
    "    those could be useful later on.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FILENAME : file\n",
    "        The csv of the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    model : object\n",
    "        MLP which can predict the outcome of NBA games\n",
    "        \n",
    "    \"\"\"\n",
    "    #Read in file, remove attempted and # and only account for % since that's more predictive in nature. \n",
    "    #*retrospectively that doesn't make sense, could be worth changing!\n",
    "    data = read_csv(FILENAME) \n",
    "    data['3P%'] = np.divide(data['3P'].values,data['3PA'].values) \n",
    "    del data['3P'],data['3PA']\n",
    "    data['FG%'] = np.divide(data['FG'].values,data['FGA'].values)\n",
    "    del data['FG'],data['FGA']\n",
    "    data['FT%'] = np.divide(data['FT'].values,data['FTA'].values)\n",
    "    del data['FT'],data['FTA']\n",
    "    data = get_dummies(data)\n",
    "    del data['VENUE_Home'],data['VENUE_Road']\n",
    "    #print(data)\n",
    "    \n",
    "\n",
    "    dat = []\n",
    "    \n",
    "    #reshape the dataset so now each colummn has roadstats and homestats concatenated into the same row, used for NN \n",
    "    \n",
    "    for i in range(len(data.values)):\n",
    "        data.values[i] = np.reshape(data.values[i],newshape=[1,len(data.values[i])])\n",
    "    for p in range(int(len(data.values)/2)):\n",
    "        fullboxgame = np.concatenate((data.values[2*p],data.values[(2*p)+1]))\n",
    "        dat.append(fullboxgame)\n",
    "    \n",
    "    #convert list to array, now possible to array operations previously not possible\n",
    "    dat = np.array(dat)   \n",
    "    \n",
    "    #now to find out the score of the game, and whether or not the spread was covered. \n",
    "    roadpts = dat[:,8] #column of all the points scored by road team \n",
    "    homepts = dat[:,21] #vice versa of above\n",
    "    print(\"roadpts\",roadpts)\n",
    "    endspreadS = roadpts-homepts  #all the final spreads of the game\n",
    "    openingspreadS = dat[:,9]  #what the predicted spread of ther game was. \n",
    "    y = []\n",
    "    \n",
    "    #Now I iterated over all these, and hot encoded the labels of each to see whether or not the spread was covered\n",
    "    #and by what team. \n",
    "    for j in range(len(endspreadS)):  \n",
    "        openspread = openingspreadS[j]\n",
    "       # print(\"this is the spread of the road team \" + str(openspread))\n",
    "        endspread = endspreadS[j]\n",
    "       # print(\"the road team won by  .. \" + str(endspread))\n",
    "       # if endspread>openspread:\n",
    "        #    y.append(np.array([0,1,0]))  #OK, now make sure this is formateed properly!\n",
    "        if openspread + endspread <0:\n",
    "            y.append(np.array([0,1,0]))  #hoem team covered\n",
    "        elif openspread + endspread >0:\n",
    "            y.append(np.array([1,0,0]))  #road covered\n",
    "        else: \n",
    "            y.append(np.array([0,0,1]))  #push!\n",
    "\n",
    "\n",
    "\n",
    "    y = np.array(y)  #same explanation as above\n",
    "    X1 = np.concatenate((dat[:,0:8],dat[:,10:21]),axis=1)  #reshaping arrays,\n",
    "    #since everything got out of order I have to mash it together myself. \n",
    "    X = np.concatenate((X1,dat[:,23:26]),axis=1)    #need to go one further column to snag HFT% \n",
    "    X_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.27)\n",
    "    #print((X[0]))\n",
    "    #print(np.shape(X[0]))\n",
    "\n",
    "    #now to construct a model \n",
    "    if sklearn: \n",
    "        model = MLPClassifier()\n",
    "        model.shuffle = True\n",
    "        model.batch_size = 25\n",
    "    #model.n_layers_ = 1000000\n",
    "    #model.n_outputs_= 1000000\n",
    "    #These don't do anything, have to adjust the layers in some different way! Keras is useful for this.\n",
    "        model.fit(X_train,y_train)\n",
    "        print(model.score(X_test,y_test))\n",
    "    if keras:\n",
    "        print(\"keras NN goes here\")\n",
    "        model=Sequential()\n",
    "        model.add(Dense(22,input_dim=np.shape(X)[1],activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(50,activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(22,activation='relu'))\n",
    "\n",
    "        model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.fit(X_train,y_train,batch_size=40,epochs=20,validation_split=.2)\n",
    "        scores = model.evaluate(X_test,y_test)\n",
    "        print(scores[1])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roadpts [ 88. 104. 129. ... 112. 101.  93.]\n",
      "keras NN goes here\n",
      "Train on 1532 samples, validate on 384 samples\n",
      "Epoch 1/20\n",
      "1532/1532 [==============================] - 1s 472us/step - loss: 0.7252 - acc: 0.6456 - val_loss: 0.6097 - val_acc: 0.7448\n",
      "Epoch 2/20\n",
      "1532/1532 [==============================] - 0s 45us/step - loss: 0.6243 - acc: 0.7226 - val_loss: 0.6708 - val_acc: 0.6615\n",
      "Epoch 3/20\n",
      "1532/1532 [==============================] - 0s 50us/step - loss: 0.6082 - acc: 0.7154 - val_loss: 0.5655 - val_acc: 0.7708\n",
      "Epoch 4/20\n",
      "1532/1532 [==============================] - 0s 51us/step - loss: 0.5873 - acc: 0.7461 - val_loss: 0.5493 - val_acc: 0.7448\n",
      "Epoch 5/20\n",
      "1532/1532 [==============================] - 0s 47us/step - loss: 0.5842 - acc: 0.7520 - val_loss: 0.5433 - val_acc: 0.7734\n",
      "Epoch 6/20\n",
      "1532/1532 [==============================] - 0s 48us/step - loss: 0.5743 - acc: 0.7480 - val_loss: 0.5398 - val_acc: 0.7474\n",
      "Epoch 7/20\n",
      "1532/1532 [==============================] - 0s 47us/step - loss: 0.5608 - acc: 0.7591 - val_loss: 0.5302 - val_acc: 0.7708\n",
      "Epoch 8/20\n",
      "1532/1532 [==============================] - 0s 50us/step - loss: 0.5705 - acc: 0.7572 - val_loss: 0.6704 - val_acc: 0.6589\n",
      "Epoch 9/20\n",
      "1532/1532 [==============================] - 0s 52us/step - loss: 0.5774 - acc: 0.7467 - val_loss: 0.5335 - val_acc: 0.7604\n",
      "Epoch 10/20\n",
      "1532/1532 [==============================] - 0s 49us/step - loss: 0.5539 - acc: 0.7591 - val_loss: 0.5321 - val_acc: 0.7656\n",
      "Epoch 11/20\n",
      "1532/1532 [==============================] - 0s 52us/step - loss: 0.5513 - acc: 0.7715 - val_loss: 0.5462 - val_acc: 0.7786\n",
      "Epoch 12/20\n",
      "1532/1532 [==============================] - 0s 51us/step - loss: 0.5623 - acc: 0.7676 - val_loss: 0.5414 - val_acc: 0.7813\n",
      "Epoch 13/20\n",
      "1532/1532 [==============================] - 0s 45us/step - loss: 0.5699 - acc: 0.7474 - val_loss: 0.5212 - val_acc: 0.7917\n",
      "Epoch 14/20\n",
      "1532/1532 [==============================] - 0s 48us/step - loss: 0.5565 - acc: 0.7761 - val_loss: 0.5961 - val_acc: 0.7135\n",
      "Epoch 15/20\n",
      "1532/1532 [==============================] - 0s 49us/step - loss: 0.5516 - acc: 0.7533 - val_loss: 0.5491 - val_acc: 0.7760\n",
      "Epoch 16/20\n",
      "1532/1532 [==============================] - 0s 50us/step - loss: 0.5671 - acc: 0.7487 - val_loss: 0.5152 - val_acc: 0.7786\n",
      "Epoch 17/20\n",
      "1532/1532 [==============================] - 0s 43us/step - loss: 0.5406 - acc: 0.7709 - val_loss: 0.5102 - val_acc: 0.7839\n",
      "Epoch 18/20\n",
      "1532/1532 [==============================] - 0s 43us/step - loss: 0.5423 - acc: 0.7715 - val_loss: 0.5186 - val_acc: 0.7682\n",
      "Epoch 19/20\n",
      "1532/1532 [==============================] - 0s 45us/step - loss: 0.5394 - acc: 0.7800 - val_loss: 0.5084 - val_acc: 0.7865\n",
      "Epoch 20/20\n",
      "1532/1532 [==============================] - 0s 47us/step - loss: 0.5392 - acc: 0.7748 - val_loss: 0.5056 - val_acc: 0.7839\n",
      "709/709 [==============================] - 0s 24us/step\n",
      "0.8025387871921147\n"
     ]
    }
   ],
   "source": [
    "#For this spreadsheet, see github. \n",
    "nbapredictor = make_network('1517_boxscores.csv',sklearn=False,keras=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function I have is for turning the current nba team statistics (either over the entire season or some stretch) into an array of the same shape and information as the one used for the box scores above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction_data(filename):\n",
    "    from pandas import read_csv,get_dummies\n",
    "    import numpy as np\n",
    "    from sklearn import cross_validation\n",
    "    data = read_csv(filename)\n",
    "    #convert to per game stats and sort columns. \n",
    "    data['ORB'] =  np.divide(data['ORB'].values,data['G'].values)\n",
    "    data['DRB'] =  np.divide(data['DRB'].values,data['G'].values)\n",
    "    data['TRB'] =  np.divide(data['TRB'].values,data['G'].values)\n",
    "    data['AST'] =  np.divide(data['AST'].values,data['G'].values)\n",
    "    data['STL'] =  np.divide(data['STL'].values,data['G'].values)\n",
    "    data['BLK'] =  np.divide(data['BLK'].values,data['G'].values)\n",
    "    data['TOV'] =  np.divide(data['TOV'].values,data['G'].values)\n",
    "    data['PF'] =  np.divide(data['PF'].values,data['G'].values)\n",
    "    teams  = data['Team']\n",
    "\n",
    "    data = data[['ORB' , 'DRB'  ,'TRB' ,  'AST' , 'PF' , 'STL' , 'TOV' , 'BLK' ,'3P%','FG%' ,'FT%']]\n",
    "    print(\"Here is every teams index value: \")\n",
    "    print(teams)\n",
    "    return teams,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def game_maker(roadteam,hometeam):\n",
    "    \"\"\"After creating a properly formated table, this concats the desired teams so they can be predicted. \n",
    "        Based on get team index # based on output of predictor, and make it the input for stats ie GSW are stats[0].\n",
    "        and so on!\n",
    "    \"\"\"\n",
    "    game = np.concatenate((roadteam,hometeam))\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is every teams index value: \n",
      "0     Golden State Warriors* \n",
      "1     Houston Rockets*       \n",
      "2     New Orleans Pelicans*  \n",
      "3     Toronto Raptors*       \n",
      "4     Cleveland Cavaliers*   \n",
      "5     Denver Nuggets         \n",
      "6     Philadelphia 76ers*    \n",
      "7     Minnesota Timberwolves*\n",
      "8     Los Angeles Clippers   \n",
      "9     Charlotte Hornets      \n",
      "10    Los Angeles Lakers     \n",
      "11    Oklahoma City Thunder* \n",
      "12    Washington Wizards*    \n",
      "13    Brooklyn Nets          \n",
      "14    Milwaukee Bucks*       \n",
      "15    Portland Trail Blazers*\n",
      "16    Indiana Pacers*        \n",
      "17    New York Knicks        \n",
      "18    Utah Jazz*             \n",
      "19    Boston Celtics*        \n",
      "20    Phoenix Suns           \n",
      "21    Detroit Pistons        \n",
      "22    Miami Heat*            \n",
      "23    Orlando Magic          \n",
      "24    Atlanta Hawks          \n",
      "25    Chicago Bulls          \n",
      "26    San Antonio Spurs*     \n",
      "27    Dallas Mavericks       \n",
      "28    Memphis Grizzlies      \n",
      "29    Sacramento Kings       \n",
      "30    League Average         \n",
      "Name: Team, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filetouse = '1718nbateamstats.csv'  #downloaded from basketball reference, and specified date \n",
    "\n",
    "teams, stats =make_prediction_data(filetouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playoff teams + Index #\n",
    "----------------------\n",
    "Warriors 0\n",
    "-----------\n",
    "Rockets 1\n",
    "-----------\n",
    "Pelicans 2\n",
    "-----------\n",
    "Raptors 3 \n",
    "-----------\n",
    "Cavs 4\n",
    "-----------\n",
    "76ers 6\n",
    "-----------\n",
    "Timberwolves 7\n",
    "-----------\n",
    "Thunder 11\n",
    "-----------\n",
    "Wizards 12\n",
    "-----------\n",
    "Bucks 14\n",
    "-----------\n",
    "Trail Blazers 15\n",
    "-----------\n",
    "Pacers 16\n",
    "-----------\n",
    "Jazz 18\n",
    "-----------\n",
    "Celtics 19\n",
    "-----------\n",
    "Heat 22\n",
    "-----------\n",
    "Spurs 26\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now or tomorrow it would be smark to analyze all the different games, a ton have already been played, could be worth investigating what my playoff record is after round 1 for a specific series\n",
    "\n",
    "\n",
    "--Some things worth noting are playoff injuries, etc. like how the sixers are never really at their full strength and same goes for warriors but whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Round2matchups\n",
    "phibos = game_maker(stats.values[6],stats.values[19]) #philly at boston\n",
    "bosphi = game_maker(stats.values[19],stats.values[6]) #vice versa\n",
    "cletor = game_maker(stats.values[4],stats.values[3])  #cleveland at toronto\n",
    "torcle = game_maker(stats.values[3],stats.values[4]) #vice versa\n",
    "utahou = game_maker(stats.values[18],stats.values[1]) #minny at houston\n",
    "houuta = game_maker(stats.values[1],stats.values[18]) #vice versa\n",
    "nopgsw = game_maker(stats.values[2],stats.values[0])  # pelicans at gsw\n",
    "gswnop = game_maker(stats.values[0],stats.values[2])\n",
    "\n",
    "gswsac = game_maker(stats.values[29],stats.values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5215818 , 0.52082324, 0.06708987],\n",
       "       [0.6409434 , 0.46189535, 0.06705207]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so who covers the various game 1s? \n",
    "nbapredictor.predict(np.array([cletor,nopgsw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8102558 , 0.3428352 , 0.03644867]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbapredictor.predict(np.array([phibos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8464921 + 0.45634875 + 0.10764076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9274949 + 0.3562849 + 0.10068643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minhou = game_maker(stats.values[7],stats.values[1]) #minny at houston\n",
    "houmin = game_maker(stats.values[1],stats.values[7]) #vice versa\n",
    "\n",
    "game1 = game_maker(stats.values[1],stats.values[18])\n",
    "game2 = game_maker(stats.values[3],stats.values[15])\n",
    "game3 = game_maker(stats.values[7],stats.values[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbapredictor.predict(np.array([game1,game2,game3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
