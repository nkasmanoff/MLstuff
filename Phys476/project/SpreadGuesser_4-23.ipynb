{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my most up to date and as annotated as possible script. For the data set, have it in you local directory (It's uploaded to github too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import dependences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense\n",
    "np.random.seed(7) #why random?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_network(FILENAME,sklearn=False,keras=False,normalize=True):\n",
    "    from pandas import read_csv,get_dummies\n",
    "    import numpy as np\n",
    "    from sklearn import cross_validation\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the csv input of all the box scores, arrange it such that the home and away teams are lined up, \n",
    "    unnecessary columns removed, and hot encoding is done. Other stuff too probably. Such as normalization, but I \n",
    "    didn't do that!\n",
    "    \n",
    "    Note that this data has already been doctored from its original form, taking out most unnecessary columns but\n",
    "    those could be useful later on.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FILENAME : file\n",
    "        The csv of the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    model : object\n",
    "        MLP which can predict the outcome of NBA games\n",
    "        \n",
    "    \"\"\"\n",
    "    #Read in file, remove attempted and # and only account for % since that's more predictive in nature. \n",
    "    #*retrospectively that doesn't make sense, could be worth changing!\n",
    "    data = read_csv(FILENAME) \n",
    "    data['3P%'] = np.divide(data['3P'].values,data['3PA'].values) \n",
    "    del data['3P'],data['3PA']\n",
    "    data['FG%'] = np.divide(data['FG'].values,data['FGA'].values)\n",
    "    del data['FG'],data['FGA']\n",
    "    data['FT%'] = np.divide(data['FT'].values,data['FTA'].values)\n",
    "    del data['FT'],data['FTA']\n",
    "    data = get_dummies(data)\n",
    "    del data['VENUE_Home'],data['VENUE_Road']\n",
    "    #print(data)\n",
    "    \n",
    "\n",
    "    dat = []\n",
    "    \n",
    "    #reshape the dataset so now each colummn has roadstats and homestats concatenated into the same row, used for NN \n",
    "    \n",
    "    for i in range(len(data.values)):\n",
    "        data.values[i] = np.reshape(data.values[i],newshape=[1,len(data.values[i])])\n",
    "    for p in range(int(len(data.values)/2)):\n",
    "        fullboxgame = np.concatenate((data.values[2*p],data.values[(2*p)+1]))\n",
    "        dat.append(fullboxgame)\n",
    "    \n",
    "    #convert list to array, now possible to array operations previously not possible\n",
    "    dat = np.array(dat)   \n",
    "    \n",
    "    #now to find out the score of the game, and whether or not the spread was covered. \n",
    "    roadpts = dat[:,8] #column of all the points scored by road team \n",
    "    homepts = dat[:,21] #vice versa of above\n",
    "    #print(\"roadpts\",roadpts)\n",
    "    endspreadS = roadpts-homepts  #all the final spreads of the game\n",
    "    openingspreadS = dat[:,9]  #what the predicted spread of ther game was. \n",
    "    y = []\n",
    "    \n",
    "    #Now I iterated over all these, and hot encoded the labels of each to see whether or not the spread was covered\n",
    "    #and by what team. \n",
    "    for j in range(len(endspreadS)):  \n",
    "        openspread = openingspreadS[j]\n",
    "       # print(\"this is the spread of the road team \" + str(openspread))\n",
    "        endspread = endspreadS[j]\n",
    "       # print(\"the road team won by  .. \" + str(endspread))\n",
    "       # if endspread>openspread:\n",
    "        #    y.append(np.array([0,1,0]))  #OK, now make sure this is formateed properly!\n",
    "        if openspread + endspread <0:\n",
    "            y.append(np.array([0,1,0]))  #home team covered\n",
    "        elif openspread + endspread >0:\n",
    "            y.append(np.array([1,0,0]))  #road covered\n",
    "        else: \n",
    "            y.append(np.array([0,0,1]))  #push!\n",
    "\n",
    "\n",
    "        \n",
    "    y = np.array(y)  #same explanation as above\n",
    "    X1 = np.concatenate((dat[:,0:8],dat[:,10:21]),axis=1)  #reshaping arrays,\n",
    "    #since everything got out of order I have to mash it together myself. \n",
    "    X = np.concatenate((X1,dat[:,23:26]),axis=1)    #need to go one further column to snag HFT% \n",
    "    if normalize:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    X_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.27)\n",
    "    #print((X[0]))\n",
    "    #print(np.shape(X[0]))\n",
    "\n",
    "    #now to construct a model \n",
    "    if sklearn: \n",
    "        model = MLPClassifier()\n",
    "        model.shuffle = True\n",
    "        model.batch_size = 25\n",
    "    #model.n_layers_ = 1000000\n",
    "    #model.n_outputs_= 1000000\n",
    "    #These don't do anything, have to adjust the layers in some different way! Keras is useful for this.\n",
    "        model.fit(X_train,y_train)\n",
    "        print(model.score(X_test,y_test))\n",
    "    if keras:\n",
    "        print(\"keras NN goes here\")\n",
    "        model=Sequential()\n",
    "        model.add(Dense(22,input_dim=np.shape(X)[1],activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(50,activation='relu'))\n",
    "        model.add(Dense(30,activation='relu'))\n",
    "        model.add(Dense(22,activation='relu'))\n",
    "\n",
    "        model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.fit(X_train,y_train,batch_size=40,epochs=20,validation_split=.2)\n",
    "        scores = model.evaluate(X_test,y_test)\n",
    "        print(scores[1])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model,scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function I have is for turning the current nba team statistics (either over the entire season or some stretch) into an array of the same shape and information as the one used for the box scores above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction_data(filename):\n",
    "    from pandas import read_csv,get_dummies\n",
    "    import numpy as np\n",
    "    from sklearn import cross_validation\n",
    "    data = read_csv(filename)\n",
    "    #convert to per game stats and sort columns. \n",
    "    data['ORB'] =  np.divide(data['ORB'].values,data['G'].values)\n",
    "    data['DRB'] =  np.divide(data['DRB'].values,data['G'].values)\n",
    "    data['TRB'] =  np.divide(data['TRB'].values,data['G'].values)\n",
    "    data['AST'] =  np.divide(data['AST'].values,data['G'].values)\n",
    "    data['STL'] =  np.divide(data['STL'].values,data['G'].values)\n",
    "    data['BLK'] =  np.divide(data['BLK'].values,data['G'].values)\n",
    "    data['TOV'] =  np.divide(data['TOV'].values,data['G'].values)\n",
    "    data['PF'] =  np.divide(data['PF'].values,data['G'].values)\n",
    "    teams  = data['Team']\n",
    "\n",
    "    data = data[['ORB' , 'DRB'  ,'TRB' ,  'AST' , 'PF' , 'STL' , 'TOV' , 'BLK' ,'3P%','FG%' ,'FT%']]\n",
    "   # print(\"Here is every teams index value: \")\n",
    "   # print(teams)\n",
    "    return teams,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def game_maker(roadteam,hometeam,scaler):\n",
    "    \"\"\"After creating a properly formated table, this concats the desired teams so they can be predicted. \n",
    "        Based on get team index # based on output of predictor, and make it the input for stats ie GSW are stats[0].\n",
    "        and so on!\n",
    "    \"\"\"\n",
    "    game = np.concatenate((roadteam,hometeam))\n",
    "    game = [game]\n",
    "    game = scaler.transform(game)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filetouse = '1718nbateamstats.csv'  #downloaded from basketball reference, and specified date \n",
    "\n",
    "teams, stats =make_prediction_data(filetouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playoff teams + Index #\n",
    "----------------------\n",
    "Warriors 0\n",
    "-----------\n",
    "Rockets 1\n",
    "-----------\n",
    "Pelicans 2\n",
    "-----------\n",
    "Raptors 3 \n",
    "-----------\n",
    "Cavs 4\n",
    "-----------\n",
    "76ers 6\n",
    "-----------\n",
    "Timberwolves 7\n",
    "-----------\n",
    "Thunder 11\n",
    "-----------\n",
    "Wizards 12\n",
    "-----------\n",
    "Bucks 14\n",
    "-----------\n",
    "Trail Blazers 15\n",
    "-----------\n",
    "Pacers 16\n",
    "-----------\n",
    "Jazz 18\n",
    "-----------\n",
    "Celtics 19\n",
    "-----------\n",
    "Heat 22\n",
    "-----------\n",
    "Spurs 26\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now or tomorrow it would be smark to analyze all the different games, a ton have already been played, could be worth investigating what my playoff record is after round 1 for a specific series\n",
    "\n",
    "\n",
    "--Some things worth noting are playoff injuries, etc. like how the sixers are never really at their full strength and same goes for warriors but whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8067700987306065\n"
     ]
    }
   ],
   "source": [
    "#returns the keras model for prediction, along with the scaling tool to normalize future data\n",
    "nbapredictor,scaler = make_network('1517_boxscores.csv',sklearn=True,keras=False,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conference finals HO LI FUK\n",
    "\n",
    "gswhou = game_maker(stats.values[0],stats.values[1],scaler)\n",
    "hougsw = game_maker(stats.values[1],stats.values[0],scaler)\n",
    "\n",
    "#\n",
    "clebos = game_maker(stats.values[4],stats.values[19],scaler)\n",
    "boscle = game_maker(stats.values[19],stats.values[4],scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbapredictor.predict(clebos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbapredictor.predict(gswhou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#west round 2\n",
    "utahou = game_maker(stats.values[18],stats.values[1],scaler) #minny at houston\n",
    "houuta = game_maker(stats.values[1],stats.values[18],scaler) #vice versa\n",
    "nopgsw = game_maker(stats.values[2],stats.values[0],scaler)  # pelicans at gsw\n",
    "gswnop = game_maker(stats.values[0],stats.values[2],scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictor.predict(gswnop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#western conference round 1\n",
    "sasgsw = game_maker(stats.values[26],stats.values[0],scaler)\n",
    "gswsas = game_maker(stats.values[0],stats.values[26],scaler)\n",
    "minhou = game_maker(stats.values[7],stats.values[1],scaler)\n",
    "houmin = game_maker(stats.values[1],stats.values[7],scaler)\n",
    "utaokc = game_maker(stats.values[18],stats.values[11],scaler)\n",
    "okcuta = game_maker(stats.values[11],stats.values[18],scaler)\n",
    "noppor = game_maker(stats.values[2],stats.values[15],scaler)\n",
    "pornop = game_maker(stats.values[15],stats.values[2],scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictor.predict(indcle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#okc uta too close in this version, gonna re run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eastern conference round 1\n",
    "miaphi = game_maker(stats.values[22],stats.values[6],scaler)\n",
    "phimia = game_maker(stats.values[6],stats.values[22],scaler)\n",
    "milbos = game_maker(stats.values[14],stats.values[19],scaler)\n",
    "bosmil = game_maker(stats.values[19],stats.values[14],scaler)\n",
    "wastor = game_maker(stats.values[12],stats.values[3],scaler)\n",
    "torwas = game_maker(stats.values[3],stats.values[12],scaler)\n",
    "\n",
    "indcle = game_maker(stats.values[16],stats.values[4],scaler)\n",
    "cleind = game_maker(stats.values[4],stats.values[16],scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictor.predict(torwas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Round2matchups\n",
    "phibos = game_maker(stats.values[6],stats.values[19],scaler) #philly at boston\n",
    "bosphi = game_maker(stats.values[19],stats.values[6],scaler) #vice versa\n",
    "cletor = game_maker(stats.values[4],stats.values[3],scaler)  #cleveland at toronto\n",
    "torcle = game_maker(stats.values[3],stats.values[4],scaler) #vice versa\n",
    "utahou = game_maker(stats.values[18],stats.values[1],scaler) #minny at houston\n",
    "houuta = game_maker(stats.values[1],stats.values[18],scaler) #vice versa\n",
    "nopgsw = game_maker(stats.values[2],stats.values[0],scaler)  # pelicans at gsw\n",
    "gswnop = game_maker(stats.values[0],stats.values[2],scaler)\n",
    "\n",
    "gswsac = game_maker(stats.values[29],stats.values[0],scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#so who covers the various game 1s? \n",
    "nbapredictor.predict(cletor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictor.predict(nopgsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.8464921 + 0.45634875 + 0.10764076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.9274949 + 0.3562849 + 0.10068643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minhou = game_maker(stats.values[7],stats.values[1]) #minny at houston\n",
    "houmin = game_maker(stats.values[1],stats.values[7]) #vice versa\n",
    "\n",
    "game1 = game_maker(stats.values[1],stats.values[18])\n",
    "game2 = game_maker(stats.values[3],stats.values[15])\n",
    "game3 = game_maker(stats.values[7],stats.values[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbapredictor.predict(np.array([game1,game2,game3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_py import team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = team.TeamList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEAGUE_ID</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>MIN_YEAR</th>\n",
       "      <th>MAX_YEAR</th>\n",
       "      <th>ABBREVIATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>1949</td>\n",
       "      <td>2017</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612738</td>\n",
       "      <td>1946</td>\n",
       "      <td>2017</td>\n",
       "      <td>BOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612739</td>\n",
       "      <td>1970</td>\n",
       "      <td>2017</td>\n",
       "      <td>CLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612740</td>\n",
       "      <td>2002</td>\n",
       "      <td>2017</td>\n",
       "      <td>NOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612741</td>\n",
       "      <td>1966</td>\n",
       "      <td>2017</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612742</td>\n",
       "      <td>1980</td>\n",
       "      <td>2017</td>\n",
       "      <td>DAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>1976</td>\n",
       "      <td>2017</td>\n",
       "      <td>DEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>1946</td>\n",
       "      <td>2017</td>\n",
       "      <td>GSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612745</td>\n",
       "      <td>1967</td>\n",
       "      <td>2017</td>\n",
       "      <td>HOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>1970</td>\n",
       "      <td>2017</td>\n",
       "      <td>LAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>1948</td>\n",
       "      <td>2017</td>\n",
       "      <td>LAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612748</td>\n",
       "      <td>1988</td>\n",
       "      <td>2017</td>\n",
       "      <td>MIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612749</td>\n",
       "      <td>1968</td>\n",
       "      <td>2017</td>\n",
       "      <td>MIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612750</td>\n",
       "      <td>1989</td>\n",
       "      <td>2017</td>\n",
       "      <td>MIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612751</td>\n",
       "      <td>1976</td>\n",
       "      <td>2017</td>\n",
       "      <td>BKN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612752</td>\n",
       "      <td>1946</td>\n",
       "      <td>2017</td>\n",
       "      <td>NYK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612753</td>\n",
       "      <td>1989</td>\n",
       "      <td>2017</td>\n",
       "      <td>ORL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612754</td>\n",
       "      <td>1976</td>\n",
       "      <td>2017</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612755</td>\n",
       "      <td>1949</td>\n",
       "      <td>2017</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612756</td>\n",
       "      <td>1968</td>\n",
       "      <td>2017</td>\n",
       "      <td>PHX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612757</td>\n",
       "      <td>1970</td>\n",
       "      <td>2017</td>\n",
       "      <td>POR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612758</td>\n",
       "      <td>1948</td>\n",
       "      <td>2017</td>\n",
       "      <td>SAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612759</td>\n",
       "      <td>1976</td>\n",
       "      <td>2017</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612760</td>\n",
       "      <td>1967</td>\n",
       "      <td>2017</td>\n",
       "      <td>OKC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612761</td>\n",
       "      <td>1995</td>\n",
       "      <td>2017</td>\n",
       "      <td>TOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>1974</td>\n",
       "      <td>2017</td>\n",
       "      <td>UTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612763</td>\n",
       "      <td>1995</td>\n",
       "      <td>2017</td>\n",
       "      <td>MEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612764</td>\n",
       "      <td>1961</td>\n",
       "      <td>2017</td>\n",
       "      <td>WAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612765</td>\n",
       "      <td>1948</td>\n",
       "      <td>2017</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00</td>\n",
       "      <td>1610612766</td>\n",
       "      <td>1988</td>\n",
       "      <td>2017</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610031</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610029</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610025</td>\n",
       "      <td>1946</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610034</td>\n",
       "      <td>1946</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610036</td>\n",
       "      <td>1946</td>\n",
       "      <td>1950</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610024</td>\n",
       "      <td>1947</td>\n",
       "      <td>1954</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610027</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610030</td>\n",
       "      <td>1949</td>\n",
       "      <td>1952</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610033</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610037</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610023</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610026</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610028</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610032</td>\n",
       "      <td>1946</td>\n",
       "      <td>1948</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>00</td>\n",
       "      <td>1610610035</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LEAGUE_ID     TEAM_ID MIN_YEAR MAX_YEAR ABBREVIATION\n",
       "0         00  1610612737     1949     2017          ATL\n",
       "1         00  1610612738     1946     2017          BOS\n",
       "2         00  1610612739     1970     2017          CLE\n",
       "3         00  1610612740     2002     2017          NOP\n",
       "4         00  1610612741     1966     2017          CHI\n",
       "5         00  1610612742     1980     2017          DAL\n",
       "6         00  1610612743     1976     2017          DEN\n",
       "7         00  1610612744     1946     2017          GSW\n",
       "8         00  1610612745     1967     2017          HOU\n",
       "9         00  1610612746     1970     2017          LAC\n",
       "10        00  1610612747     1948     2017          LAL\n",
       "11        00  1610612748     1988     2017          MIA\n",
       "12        00  1610612749     1968     2017          MIL\n",
       "13        00  1610612750     1989     2017          MIN\n",
       "14        00  1610612751     1976     2017          BKN\n",
       "15        00  1610612752     1946     2017          NYK\n",
       "16        00  1610612753     1989     2017          ORL\n",
       "17        00  1610612754     1976     2017          IND\n",
       "18        00  1610612755     1949     2017          PHI\n",
       "19        00  1610612756     1968     2017          PHX\n",
       "20        00  1610612757     1970     2017          POR\n",
       "21        00  1610612758     1948     2017          SAC\n",
       "22        00  1610612759     1976     2017          SAS\n",
       "23        00  1610612760     1967     2017          OKC\n",
       "24        00  1610612761     1995     2017          TOR\n",
       "25        00  1610612762     1974     2017          UTA\n",
       "26        00  1610612763     1995     2017          MEM\n",
       "27        00  1610612764     1961     2017          WAS\n",
       "28        00  1610612765     1948     2017          DET\n",
       "29        00  1610612766     1988     2017          CHA\n",
       "30        00  1610610031     1946     1946         None\n",
       "31        00  1610610029     1948     1948         None\n",
       "32        00  1610610025     1946     1949         None\n",
       "33        00  1610610034     1946     1949         None\n",
       "34        00  1610610036     1946     1950         None\n",
       "35        00  1610610024     1947     1954         None\n",
       "36        00  1610610027     1949     1949         None\n",
       "37        00  1610610030     1949     1952         None\n",
       "38        00  1610610033     1949     1949         None\n",
       "39        00  1610610037     1949     1949         None\n",
       "40        00  1610610023     1949     1949         None\n",
       "41        00  1610610026     1946     1946         None\n",
       "42        00  1610610028     1946     1946         None\n",
       "43        00  1610610032     1946     1948         None\n",
       "44        00  1610610035     1946     1946         None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlseasons =team.TeamSeasons(team_id=1610612738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = team.TeamGeneralSplits(team_id=1610612738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eh.overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trsdf = atlseasons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.4  , 35.1  , 44.5  , 19.7  ,  7.4  , 14.   ,  4.5  ,  0.377,\n",
       "         0.45 ,  0.771]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['OREB','DREB','REB','PF','STL','TOV','BLK','FG3_PCT','FG_PCT','FT_PCT']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
