{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will construct a CNN within tensorflow to identify the type of flower, won't actually be running this code here though becuase it requires a GPU, so that'll happen on a remote server but let's get the basics down here!\n",
    "\n",
    "\n",
    "https://www.kaggle.com/alxmamaev/flowers-recognition\n",
    "\n",
    "Should now only read in jpegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I\n",
    "------\n",
    "Making the dataset\n",
    "\n",
    "Here I'm making the arrays of each flower type, defining their label, and then making a function to find the max height and width.\n",
    "\n",
    "Not sure how to find the \"nominal\" height and width, but the code to find it is in here provided you know what you're looking for (I don't). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd flowers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making my daisy set in here\n",
    "\n",
    "X_daisies = []\n",
    "y_daisies = []\n",
    "for file in os.listdir('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/daisy'):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img = cv2.imread('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/daisy/' + file)\n",
    "        X_daisies.append(img)\n",
    "        y_daisies.append([1,0,0,0,0])\n",
    "\n",
    "        \n",
    "        \n",
    "#Now for dandelions\n",
    "X_dandelions = []\n",
    "y_dandelions = []\n",
    "for file in os.listdir('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/dandelion'):\n",
    "    #print(filename)\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img = cv2.imread('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/dandelion/' + file)\n",
    "        X_dandelions.append(img)\n",
    "        y_dandelions.append([0,1,0,0,0])\n",
    "\n",
    "#Now for roses\n",
    "X_roses = []\n",
    "y_roses = []\n",
    "for file in os.listdir('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/rose'):\n",
    "    #print(filename)\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img = cv2.imread('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/rose/' + file)\n",
    "        X_roses.append(img)\n",
    "        y_roses.append([0,0,1,0,0])\n",
    "\n",
    "#Now for sunflowers\n",
    "X_sunflowers = []\n",
    "y_sunflowers = []\n",
    "for file in os.listdir('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/sunflower'):\n",
    "    #print(filename)\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img = cv2.imread('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/sunflower/' + file)\n",
    "        X_sunflowers.append(img)\n",
    "        y_sunflowers.append([0,0,0,1,0])\n",
    "\n",
    "#Last but not least, tulips\n",
    "X_tulips = []\n",
    "y_tulips = []\n",
    "for file in os.listdir('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/tulip'):\n",
    "    #print(filename)\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img = cv2.imread('/Users/noahkasmanoff/MachLearn/Phys476/hw4/flowers/tulip/' + file)\n",
    "        X_tulips.append(img)\n",
    "        y_tulips.append([0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now created arrays for each of the flowers, now merging them all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging all at once. 4236 flowers in all. \n",
    "\n",
    "X_args = (X_daisies,X_dandelions,X_roses,X_sunflowers,X_tulips)\n",
    "y_args = (y_daisies,y_dandelions,y_roses,y_sunflowers,y_tulips)\n",
    "X_flowers = np.concatenate(X_args)\n",
    "y_flowers = np.concatenate(y_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the block below we can identify shitty data. Delete here. \n",
    "Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxone = 0\n",
    "#Don't make this for just daisies, do it for all flowers and then you can make the black space\n",
    "for i in range(len(X_flowers)):\n",
    "    flower = X_flowers[i]\n",
    "    height = np.shape(flower)[1] #*np.shape(daisy)[1]\n",
    "    #Some er\n",
    "    #print(i)\n",
    "    if height>maxone:\n",
    "        print(i)\n",
    "        print(\"hmax  = \" +str(height))\n",
    "        maxone = height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxone = 0\n",
    "#Don't make this for just daisies, do it for all flowers and then you can make the black space\n",
    "for i in range(len(X_flowers)):\n",
    "    flower = X_flowers[i]\n",
    "    width = np.shape(flower)[0] #*np.shape(daisy)[1]\n",
    "    #Some er\n",
    "    #print(i)\n",
    "    if width>maxone:\n",
    "        print(i)\n",
    "        print(\"wmax  = \" +str(width))\n",
    "        maxone = width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images\n",
    "def resize_with_pad(img, img_size):\n",
    "    height, width, _ = img.shape\n",
    "    ratio = img_size / max(height, width)\n",
    "   # print(ratio)\n",
    "    if ratio < 1:\n",
    "        img = cv2.resize(img, (int(ratio * width), int(ratio * height)))\n",
    "    padding = ((img_size - img.shape[0], 0), (img_size - img.shape[1],0), (0,0))\n",
    "\n",
    "    return np.pad(img, padding, 'constant')\n",
    "\n",
    "\n",
    "for i in range(0,len(X_flowers)):\n",
    "    \n",
    "    X_flowers[i] = resize_with_pad(X_flowers[i], 360)\n",
    "   # X_flowers[i] = np.array(X_flowers[i]).reshape(1, 360,360,3) useless!\n",
    "    #Extra line here reshapes to accept batch size as a dimension of accepted pics at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really sure what this error is, probs worth investigating, but it looks like the max height is 640. Keep in mind this data set isn't clean, there's like a python script inside it and otehr things that could explain this issue and will go away once removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making this data set the rest is using a CNN which is conveniently explained how to make below!\n",
    "\n",
    "\n",
    "So basically the cells above convert this into an array, (1066, 1599, 3)  3 representing RGB vals, so that's on track with this flower recog software\n",
    "\n",
    "Next up is to design the CNN... \n",
    "\n",
    "Pulling from Aymerican Dreams...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_flowers[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should convert vals to float 32, will get on that later.\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 1\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = np.shape(X_flowers[0])[0]*np.shape(X_flowers[0])[1]  # Flower data input (img shape: x*y)\n",
    "num_classes = 5 #total flower classes, daisy, etc. 5 total \n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    \n",
    "    ###### Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 360,360 , 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = next_batch(batch_size,X_flowers,y_flowers)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_flowers,\n",
    "                                      Y: y_flowers}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "129600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_flowers[0])[0]*np.shape(X_flowers[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "360*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
